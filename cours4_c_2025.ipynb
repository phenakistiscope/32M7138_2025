{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9b7512f-4c6d-4b70-98a5-912598d88881",
   "metadata": {},
   "source": [
    "# L’image dans la machine II - deep learning\n",
    "\n",
    "## Du pixel aux images - 32M7138\n",
    "\n",
    "*Printemps 2025 - Université de Genève*\n",
    "\n",
    "*Adrien Jeanrenaud (adrien.jeanrenaud@unige.ch)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569d0701-a7a4-45bc-9237-a36a92e623e2",
   "metadata": {},
   "source": [
    "## La détection sémantique grâce à l'apprentissage profond\n",
    "\n",
    "\n",
    "### Qu'est-ce que l'apprentissage profond?\n",
    "\n",
    "L'apprentissage profond pour les images consiste à former des réseaux de neurones artificiels pour reconnaître et extraire des informations à partir d'images. Ces réseaux apprennent à détecter des motifs visuels complexes, permettant par exemple la reconnaissance d'objets, la segmentation d'éléments dans une image ou encore la génération de contenu visuel.\n",
    "\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fi0.wp.com%2Fdevelopersbreach.com%2Fwp-content%2Fuploads%2F2020%2F08%2Fcnn_banner.png%3Ffit%3D1200%252C564%26ssl%3D1&f=1&nofb=1&ipt=210f2ef4bf2dc42949511533ba31820f74019d5bf5e49e6fc99d5dd612d5877e&ipo=images\" title=\"cnn\"/>\n",
    "\n",
    "\n",
    "Dans les réseaux de neurones convolutionnels, les filtres aident à détecter des motifs dans les images. Ils agissent comme des détecteurs de caractéristiques, captant des éléments tels que des bords, textures ou formes. En se superposant à l'image, ces filtres permettent au réseau de neurones d'extraire des informations clés pour reconnaître et interpréter son contenu.\n",
    "\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fwww.mdpi.com%2Finformation%2Finformation-10-00375%2Farticle_deploy%2Fhtml%2Fimages%2Finformation-10-00375-g002.png&f=1&nofb=1&ipt=2131ac2d9adeec1a01b6e21dd5aca4b2ffd370609b7a1f4bd8dbf238c7c5734b&ipo=images\" title=\"conv\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dce29bb-9871-43a6-a622-39f15011d1ca",
   "metadata": {},
   "source": [
    "### 3.2 Qu'est-ce qu'un algorithme d'apprentissage profond?\n",
    "\n",
    "Il existe plusieurs algorithmes d'apprentissage profond utilisés en traitement d'images. Parmi les plus populaires :\n",
    "\n",
    "> * Réseaux de neurones convolutionnels (CNN) : Les CNN utilisent des filtres pour extraire progressivement des caractéristiques des images, en passant de traits simples (comme les bords) à des attributs complexes pour la classification ou la détection d'objets.\n",
    "Ils sont spécifiquement conçus pour traiter des données structurées en grille comme les images. Les CNN sont largement utilisés pour la classification d'images, la détection d'objets, la segmentation sémantique et la génération d'images.\n",
    "\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Ftse4.mm.bing.net%2Fth%3Fid%3DOIP.F5A9npwC1qzLEn-rV59WlAHaDq%26pid%3DApi&f=1&ipt=b681764f327db8665c1f7bc338f6f0ae270e6d84572f2ba13082962b4d320968&ipo=images\" title=\"cnn2\"/>\n",
    "\n",
    "> * Réseaux antagonistes génératifs (GAN) : Les GAN consistent en deux réseaux adverses, un générateur créant des images et un discriminateur les évaluant ; ils s'améliorent mutuellement pour produire des images réalistes.\n",
    "Ils sont utilisés pour créer des images synthétiques réalistes et pour l'augmentation de données.\n",
    "\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fmiro.medium.com%2Fmax%2F8000%2F1*et3fMPDclTv6ZQSf1xbkag.jpeg&f=1&nofb=1&ipt=7e8b028485afc19c8094e8b345278b0c2ade5ea51ea58433a0f1f8cedd8c436d&ipo=images\" title=\"gan\"/>\n",
    "\n",
    "> * Réseaux neuronaux récurrents (RNN) : les RNN utilisent des connexions cycliques pour traiter des données séquentielles comme des vidéos, prédisant ou générant des images basées sur des séquences précédentes.\n",
    "\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fmiro.medium.com%2Fmax%2F1400%2F1*5bjD7kmtaJI-n3qztBC2Ig.png&f=1&nofb=1&ipt=4abd920e335f1c5f5da5ec46903edd21f74d2c661fdb1c9dc3566ad84c434804&ipo=images\" title=\"rnn\"/>\n",
    "\n",
    "\n",
    "#### En termes d'applications en traitement d'images :\n",
    "\n",
    "> * Classification d'images : Les CNN sont utilisés pour identifier et classer des objets dans des images, comme dans les applications de reconnaissance faciale, la détection de maladies à partir d'images médicales, etc.\n",
    "\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fmiro.medium.com%2Fmax%2F4096%2F1*nR5QCdmqUnvU2JFBu2Xa-Q.png&f=1&nofb=1&ipt=ea3c1fbb7dc192ac04b0a1fe2c485896118571d8ca24ba5b5c75b1e332fc3258&ipo=images\" title=\"ci\"/>\n",
    "\n",
    "> * Détection d'objets : Les CNN, notamment avec des architectures comme YOLO (You Only Look Once) ou SSD (Single Shot MultiBox Detector), sont employés pour localiser et identifier plusieurs objets dans une seule image.\n",
    "\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fprojectgurukul.org%2Fwp-content%2Fuploads%2F2022%2F01%2Fyolo-cnn.webp&f=1&nofb=1&ipt=d4e9d87afc23b33c4f32c4c5ca7714f17f1fa705951828f11e77ec5818d9e15e&ipo=images\" title=\"do\"/>\n",
    "\n",
    "> * Segmentation sémantique : Les réseaux tels que U-Net sont utilisés pour segmenter chaque pixel d'une image en catégories spécifiques, comme la délimitation des organes dans des images médicales.\n",
    "\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fi.redd.it%2Fapse69opd3a61.jpg&f=1&nofb=1&ipt=4af80a8b267ff98907ed31358c760f7de9a04027d1658ccd6939241275e62f96&ipo=images\" title=\"rnn\"/>\n",
    "\n",
    "> * Génération d'images : Les GAN sont utilisés pour générer des images réalistes, comme la génération de visages, de paysages ou même de créations artistiques.\n",
    "\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fventurebeat.com%2Fwp-content%2Fuploads%2F2019%2F12%2F1_BU2GnLJF1AcrkhvbCHdppw-e1577390193648.jpeg%3Fw%3D1200%26strip%3Dall&f=1&nofb=1&ipt=67e8f68acbf4764ea54315b9863530bd32bac48a175fe184f8c9ab91aebc262f&ipo=images\" title=\"rnn\"/>\n",
    "\n",
    "\n",
    "Ces algorithmes sont appliqués dans de nombreux domaines, de la médecine à la vision par ordinateur en passant par la surveillance et la robotique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566c1247-58c4-411b-9083-f1f699f88b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Travailler sur Google Colab\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd60016-20f3-4f7f-bc23-937b24f1c671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import shutil\n",
    "%pip install tensorflow_hub\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77497329-4845-47e9-8d62-fd5a4096ba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choix dans le modèle de détection\n",
    "\n",
    "detector = hub.load(\"https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_320x320/1\") #https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet_v2/MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4370a95-ca08-4daf-b943-e38fc4d72c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des noms de classes\n",
    "classe = np.array([\n",
    "    'background', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n",
    "    'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant',\n",
    "    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
    "    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack',\n",
    "    'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
    "    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass',\n",
    "    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "    'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',\n",
    "    'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
    "    'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\n",
    "    'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n",
    "    'hair drier', 'toothbrush'])\n",
    "\n",
    "classes = np.concatenate([classe, np.repeat('none', 100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a54a07-adf5-4602-8e32-5d555683c58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DownURL = \"https://c1.staticflickr.com/5/4124/5223260186_e156eca2f8_b.jpg\" # choix de l'URL\n",
    "img_data = requests.get(DownURL).content # télécharger\n",
    "with open('gnv.jpg', 'wb') as handler: # définir le fichier et son chemin\n",
    "    handler.write(img_data) # enregistrer l'image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53debaf-8b75-4d81-ae48-c9de150d4da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('gnv.jpg')\n",
    "# Convertir l'image de BGR en RGB\n",
    "image= cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5ead8f-9773-4036-958f-ad70f8403f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenir les prédictions\n",
    "input_tensor = tf.convert_to_tensor(image) # Cette ligne de code convertit l'image en un tenseur TensorFlow. TensorFlow est un framework d'apprentissage automatique qui permet de travailler avec des tenseurs, qui sont des tableaux multidimensionnels.\n",
    "input_tensor = input_tensor[tf.newaxis, ...] # Cette ligne de code ajoute une nouvelle dimension au début du tenseur. Cette dimension est utilisée pour indiquer le nombre d'images que nous passons au modèle.\n",
    "detections = detector(input_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4971ac-3a5c-447a-827e-1d19ce8f905c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupérer les détections de la première image\n",
    "detections_boxes = detections['detection_boxes'][0].numpy()\n",
    "detection_scores = detections['detection_scores'][0].numpy()\n",
    "detection_classes = detections['detection_classes'][0].numpy().astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3f089a-a5a7-415f-a943-adef9eef6210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenir les dimensions de l'image\n",
    "height, width, _ = image.shape\n",
    "\n",
    "# Parcourir les détections et dessiner les boîtes englobantes\n",
    "for i in range(len(detections)):\n",
    "    # Ignorer les détections avec une confiance supérieure à 0.3\n",
    "    if detection_scores[i] < 0.3:\n",
    "        continue\n",
    "\n",
    "    # Obtenir les coordonnées de la boîte englobante\n",
    "    box = detections_boxes[i] * np.array([height, width, height, width])\n",
    "    box = box.astype(np.int32)\n",
    "\n",
    "    # Dessiner la boîte englobante sur l'image\n",
    "    cv2.rectangle(image, (box[1], box[0]), (box[3], box[2]), (0, 255, 0), thickness=2)\n",
    "    plt.text(box[1], box[0] - 10, f\"{classes[detection_classes[i]]} : {detection_scores[i]*100:.2f}%\", color='red')\n",
    "\n",
    "# Afficher l'image avec les boîtes englobantes\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcad02ca-ab4e-459e-8e9f-18c968f11d11",
   "metadata": {},
   "source": [
    "### Décomposer une image grâce aux \"transformers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb4ba3a-1623-484b-8451-ea6a15d7fbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "#!pip install transformers[sklearn] --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6febdbf3-7bc9-40de-ba1f-4d7bf3da7570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/google/vit-base-patch16-224\n",
    "!pip install transformers\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = 'https://c1.staticflickr.com/5/4124/5223260186_e156eca2f8_b.jpg'\n",
    "\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674d0069-d8b1-4c70-8a0a-450a727c46ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image",
   "language": "python",
   "name": "image"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
