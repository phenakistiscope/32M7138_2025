{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6140cc3-b9f5-44a5-82cb-be60bddd9959",
   "metadata": {},
   "source": [
    "# Extra - introduction au scraping\n",
    "\n",
    "## Du pixel aux images - 32M7138\n",
    "\n",
    "*Printemps 2025 - Université de Genève*\n",
    "\n",
    "*Adrien Jeanrenaud (adrien.jeanrenaud@unige.ch)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050d4afc-ac3a-4177-b1ab-0643d6bf0048",
   "metadata": {},
   "source": [
    "## **Plan du cours**\n",
    "\n",
    "> **Récupération automatique des données (scraping)**\n",
    "> * Les librairies nécessaire\n",
    "> * Chercher les données dans une page HTML d'après une URL\n",
    "> * Lister toutes les pages\n",
    "> * Automatiser la récupération \n",
    "> * Sauvegarder au format CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cdedf4-561b-4be9-9a8c-5accfcca9617",
   "metadata": {},
   "source": [
    "### Les librairies nécessaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecef0e44-fa8e-45e6-86e7-deab4bd9c19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importer librairies\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbd7fa8-358f-4a51-a893-fdf3fed06e98",
   "metadata": {},
   "source": [
    "### Chercher les données dans une page HTML d'après une URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1ccc3f-792d-4653-a41b-5448a3890c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lien que vous souhaitez télécharger\n",
    "url = \"https://archive.org/details/crash-magazine-01\"\n",
    "\n",
    "# Télécharger le contenu HTML\n",
    "response = requests.get(url)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ec963b-a29f-4da8-a9b4-c15e0921a1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier si la requête a réussi (code 200)\n",
    "if response.status_code == 200:\n",
    "    # Récupérer le contenu HTML\n",
    "    html_content = response.text\n",
    "    print(html_content)\n",
    "else:\n",
    "    print(f\"Erreur {response.status_code} lors du téléchargement du lien.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a75b85a-8d66-4238-8254-2ea9565a6739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# date de publication\n",
    "\n",
    "<dl class=\"metadata-definition\">\n",
    "        <dt>Publication date</dt>\n",
    "        <dd class=\"\">\n",
    "          <a href=\"/search.php?query=date:1984-02\">\n",
    "            <span itemprop=\"datePublished\">1984-02</span>\n",
    "        </a>\n",
    "                </dd>\n",
    "    </dl>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f080e252-42b5-40f8-b98d-f5b2df118a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiliser BeautifulSoup pour analyser le HTML\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Extraire le titre de la page\n",
    "title = soup.title.text if soup.title else \"Titre non trouvé\"\n",
    "\n",
    "# Extraire la date de publication\n",
    "publication_date_tag = soup.find(\"dt\", text=\"Publication date\")\n",
    "publication_date = publication_date_tag.find_next(\"span\", {\"itemprop\": \"datePublished\"}).text if publication_date_tag else \"Date non trouvée\"\n",
    "print(publication_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f06859-e2ae-4c5e-9496-91665ebdbdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# balise pour le pdf\n",
    "\n",
    "<a class=\"format-summary download-pill\" href=\"/download/crash-magazine-01/Crash_01_Feb_1984.pdf\" title=\"\" data-toggle=\"tooltip\" data-placement=\"auto left\" data-container=\"body\" data-original-title=\"31.8M\">\n",
    "                PDF                <span class=\"iconochive-download\" aria-hidden=\"true\"></span><span class=\"icon-label sr-only\">download</span>              </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352b3007-7782-4fbc-ad1b-5351b38cf53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lien pour le pdf\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Extraire le lien vers le fichier PDF\n",
    "pdf_link_tag = soup.find(\"a\", {\"class\": \"stealth\",\"title\": \"31.8M\", \"href\": lambda href: href and \"download\" in href})\n",
    "pdf_link = pdf_link_tag[\"href\"] if pdf_link_tag else \"Lien PDF non trouvé\"\n",
    "\n",
    "print(pdf_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895ea661-c315-403a-a52f-4d977e09246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# récupérer le titre\n",
    "\n",
    "# Extraire l'identificateur\n",
    "identifier_tag = soup.find(\"dt\", text=\"Identifier\")\n",
    "identifier = identifier_tag.find_next(\"span\", {\"itemprop\": \"identifier\"}).text if identifier_tag else \"Identificateur non trouvé\"\n",
    "print(identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9b2b0c-7689-44ce-ab0f-a7924477a505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stocker les informations dans un tableur structuré\n",
    "\n",
    "df = pd.DataFrame({\"Titre\": [identifier],\n",
    "                       \"Date_publication\": [publication_date],\n",
    "                       \"Lien_PDF\": [pdf_link]})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19762c39-33e3-47a1-a2bf-f01ede2ac522",
   "metadata": {},
   "source": [
    "### Lister toutes les pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48966fa3-07af-4bfb-90cf-5772e5038920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Générer la liste des URLs de 01 à 100\n",
    "base_url = \"https://archive.org/details/crash-magazine-{}\"\n",
    "urls = [base_url.format(str(i).zfill(2)) for i in range(1, 21)]\n",
    "\n",
    "# Vérifier les URLs qui retournent un code 200\n",
    "valid_urls = []\n",
    "\n",
    "for url in urls:\n",
    "    response = requests.head(url)  # Utilisation de HEAD pour économiser la bande passante\n",
    "    if response.status_code == 200:\n",
    "        valid_urls.append(url)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "print(len(valid_urls))\n",
    "print(urls[:2])  # Affiche les 5 premières URLs\n",
    "print(urls[-2:]) # Affiche les 5 dernières URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae040a23-f60b-4d61-ba73-e8eb7573b3ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "824bdaf3-0b2e-4b8f-8031-7c33c92c8c34",
   "metadata": {},
   "source": [
    "### Automatiser la récupération "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd34ed5-2867-4616-a0c2-db4bf5dce3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste pour stocker les résultats\n",
    "data_list = []\n",
    "\n",
    "# Parcourir chaque URL et extraire les informations\n",
    "for url in urls[:3]:\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print(f\"Extraction des données depuis {url}\")\n",
    "\n",
    "        # Analyser le HTML avec BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extraire le titre de la page\n",
    "        title = soup.title.text.strip() if soup.title else \"Titre non trouvé\"\n",
    "        title = title.split(\":\")[0]\n",
    "\n",
    "        # Extraire la date de publication\n",
    "        publication_date_tag = soup.find(\"dt\", text=\"Publication date\")\n",
    "        publication_date = publication_date_tag.find_next(\"span\", {\"itemprop\": \"datePublished\"}).text.strip() if publication_date_tag else \"Date non trouvée\"\n",
    "\n",
    "        # Extraire l'identificateur\n",
    "        identifier_tag = soup.find(\"dt\", text=\"Identifier\")\n",
    "        identifier = identifier_tag.find_next(\"span\", {\"itemprop\": \"identifier\"}).text.strip() if identifier_tag else \"Identificateur non trouvé\"\n",
    "\n",
    "        # Extraire le lien du PDF\n",
    "        #pdf_link_tag = soup.find(\"a\", {\"class\": \"stealth\", \"href\": lambda href: href and \"download\" in href})\n",
    "        #pdf_link = \"https://archive.org\" + pdf_link_tag[\"href\"] if pdf_link_tag else \"Lien PDF non trouvé\"\n",
    "\n",
    "        pdf_link_tag = soup.find(\"a\", {\"class\": \"stealth\"}, href=lambda href: href and href.endswith(\".pdf\"))\n",
    "    \n",
    "        if pdf_link_tag:\n",
    "            pdf_link = pdf_link_tag[\"href\"]\n",
    "            # Compléter l'URL si elle est relative\n",
    "            if pdf_link.startswith(\"/\"):\n",
    "                pdf_link = \"https://archive.org\" + pdf_link\n",
    "        else:\n",
    "            pdf_link = \"Lien PDF non trouvé\"\n",
    "    \n",
    "        \n",
    "        # Stocker les données\n",
    "        data_list.append({\n",
    "            \"URL\": url,\n",
    "            \"Titre\": title,\n",
    "            \"Date de publication\": publication_date,\n",
    "            \"Identificateur\": identifier,\n",
    "            \"Lien PDF\": pdf_link\n",
    "        })\n",
    "\n",
    "    else:\n",
    "        print(f\"{url} inaccessible (code {response.status_code})\")\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"\\n Résultats :\")\n",
    "for data in data_list:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f79656-8f08-4116-9982-5aa212789c93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d8991b9-a947-443c-8226-154f1587e4fb",
   "metadata": {},
   "source": [
    "### Sauvegarder au format CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516f823b-4057-4858-8137-8ef2139e7703",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_list, columns=[\"URL\", \"Titre\", \"Date de publication\", \"Identificateur\", \"Lien PDF\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8727aefb-f438-45f6-90e6-b4a5dc5b7f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"crash_magazines.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc0895d-bc73-4f0e-a6af-9cae6774f532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# télécharger les pdfs\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "# Création du dossier pour stocker les PDFs\n",
    "pdf_folder = \"pdfs\"\n",
    "os.makedirs(pdf_folder, exist_ok=True)\n",
    "\n",
    "# Parcourir chaque ligne du DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    pdf_link = row[\"Lien PDF\"]\n",
    "    identifier = row[\"Identificateur\"]\n",
    "    pdf_filename = f\"{pdf_folder}/{identifier}.pdf\"\n",
    "\n",
    "    # Vérifier si le lien PDF est valide\n",
    "    if pdf_link != \"Lien PDF non trouvé\" and isinstance(pdf_link, str) and pdf_link.startswith(\"http\"):\n",
    "        print(f\"Téléchargement de {pdf_filename}...\")\n",
    "\n",
    "        # Télécharger le PDF\n",
    "        pdf_response = requests.get(pdf_link, stream=True)\n",
    "        if pdf_response.status_code == 200:\n",
    "            with open(pdf_filename, \"wb\") as pdf_file:\n",
    "                for chunk in pdf_response.iter_content(1024):\n",
    "                    pdf_file.write(chunk)\n",
    "            print(f\"PDF enregistré : {pdf_filename}\")\n",
    "        else:\n",
    "            print(f\"Erreur lors du téléchargement du PDF pour {identifier}\")\n",
    "    else:\n",
    "        print(f\"⚠ Aucun lien PDF valide pour {identifier}\")\n",
    "\n",
    "print(\"\\nTéléchargement terminé. Tous les PDFs sont dans le dossier 'pdfs/'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c98608-f055-4bec-a17d-a6f81f13b4da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image",
   "language": "python",
   "name": "image"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
